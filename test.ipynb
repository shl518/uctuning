{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import ipdb\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "def clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# mask -------------------------------\n",
    "def subsequent_mask(size):\n",
    "    ipdb.set_trace()\n",
    "    attn_shape = (1,size,size)\n",
    "    # torch.triu创建一个上三角矩阵\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "\n",
    "# Attention ----------------------------\n",
    "def attention(query,key,value,mask=None,dropout=None):\n",
    "    # query b h l d_k\n",
    "    d_k = query.size(-1)\n",
    "    # 为什么？ 看query和key的形状，这里要看形状\n",
    "    # score b h l l\n",
    "    scores = torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k) # 1 8 10 10\n",
    "    if mask is  not None: # mask 1 1 1 10\n",
    "        scores = scores.masked_fill(mask == 0,-1e9) # 返回新的张量，如果修改原始张量则是用in-place版本masked_fill_()\n",
    "    p_attn = scores.softmax(dim=-1) # 以最后一个作softmax，形状b h l l 1 8 10 10\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn) #这里为啥要dropout\n",
    "    return torch.matmul(p_attn,value),p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,h,d_model,dropout=0.1):\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model,d_model),4) #W_q W_k W_v W_o\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 先project\n",
    "        query,key,value = [ \n",
    "            # h x d_model\n",
    "            # lin(x)  b l d_model  b l h d_k 将特征维度按照头切分 -> b h l dk\n",
    "            lin(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2)\n",
    "            for lin, x in zip(self.linears,(query,key,value))\n",
    "        ]\n",
    "\n",
    "        # query b l h d_k -> b h l d_k\n",
    "        \n",
    "        x,self.attn = attention(query,key,value,mask=mask,dropout=self.dropout)\n",
    "\n",
    "        x = (\n",
    "            x.transpose(1,2)\n",
    "            .contiguous()\n",
    "            .view(nbatches,-1,self.h*self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder,decoder,src_embed,tgt_embed,generator):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self,src,tgt,src_mask,tgt_mask):\n",
    "        ipdb.set_trace()\n",
    "        return self.decode(self.encode(src,src_mask),src_mask,tgt,tgt_mask)\n",
    "\n",
    "    def encode(self,src,src_mask):\n",
    "        return self.encoder(self.src_embed(src),src_mask)\n",
    "\n",
    "    # memory是干啥的\n",
    "    def decode(self,memory,src_mask,tgt,tgt_mask):\n",
    "        ipdb.set_trace()\n",
    "        return self.decoder(self.tgt_embed(tgt),memory,src_mask,tgt_mask)\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,vocab):\n",
    "        super(Generator,self).__init__()\n",
    "        self.proj = nn.Linear(d_model,vocab)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ipdb.set_trace()\n",
    "        return log_softmax(self.proj(x),dim=-1) \n",
    "\n",
    "\n",
    "# --------------------------------Encoder-----------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    # encoder 是6个 layer的堆叠\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size) ## 为什么是layer.size?????,这是啥玩意\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,features,eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True) #为啥是-1？\n",
    "        std = x.std(-1,keepdim=True)\n",
    "        return self.a_2*(x-mean)/(std+self.eps)+self.b_2\n",
    "\n",
    "    \n",
    "# 残差连接\n",
    "class SublayerConnection(nn.Module):\n",
    "\n",
    "    def __init__(self,size,dropout):\n",
    "        super(SublayerConnection,self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout) # drop_out 是一个数值丢弃率\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "#---->norm-->attention->add->norm->mlp->add\n",
    "#   |                    |  |              |\n",
    "#    --------------------    --------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,feed_forward,dropout):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.self_attn =  self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size,dropout),2)\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        x = self.sublayer[0](x,lambda x: self.self_attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x,self.feed_forward)\n",
    "\n",
    "# --------------------------------Decoder-----------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layers = clones(layer,N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,memory,src_mask,tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self,x,memory,src_mask,tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x,lambda x: self.self_attn(x,x,x,tgt_mask)) ## 为什么，这里的两个mask起到什么作用,推理时x是单个词汇吗\n",
    "        x = self.sublayer[1](x,lambda x: self.src_attn(x,m,m,src_mask)) # 为什么memory是什么\n",
    "        return self.sublayer[2](x,self.feed_forward)\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    \n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        ipdb.set_trace()\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        ipdb.set_trace()\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "    \n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "\n",
    "show_example(run_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
